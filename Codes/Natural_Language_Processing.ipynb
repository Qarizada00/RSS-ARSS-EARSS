{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5\n",
    "#The algorithm for compound words are taken from mentioned website above\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import neattext.functions as nfx\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"C:/Users/Nasibullah Qarizada/Desktop/data science/term project/AllDataARSAA.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['features missing?'] = data['features missing?'].apply(nfx.remove_hashtags)\n",
    "data['features missing?'] = data['features missing?'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['features missing?'] = data['features missing?'].apply(nfx.remove_urls)\n",
    "data['features missing?'] = data['features missing?'].apply(nfx.remove_puncts)\n",
    "data['features missing?'] = data['features missing?'].apply(nfx.remove_multiple_spaces)\n",
    "data['features missing?'] = data['features missing?'].apply(nfx.remove_stopwords)\n",
    "data['tokenized'] = data['features missing?'].apply(lambda x: x.split())\n",
    "data['features missing?'] = data['features missing?'].apply(lambda x: x.lower())\n",
    "data['features missing?'] = data['features missing?'].astype(str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['room for improvement?'] = data['room for improvement?'].apply(nfx.remove_hashtags)\n",
    "data['room for improvement?'] = data['room for improvement?'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['room for improvement?'] = data['room for improvement?'].apply(nfx.remove_urls)\n",
    "data['room for improvement?'] = data['room for improvement?'].apply(nfx.remove_puncts)\n",
    "data['room for improvement?'] = data['room for improvement?'].apply(nfx.remove_multiple_spaces)\n",
    "data['room for improvement?'] = data['room for improvement?'].apply(nfx.remove_stopwords)\n",
    "data['tokenized'] = data['room for improvement?'].apply(lambda x: x.split())\n",
    "data['room for improvement?'] = data['room for improvement?'].apply(lambda x: x.lower())\n",
    "data['room for improvement?'] = data['room for improvement?'].astype(str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(nfx.remove_hashtags)\n",
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(nfx.remove_urls)\n",
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(nfx.remove_puncts)\n",
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(nfx.remove_multiple_spaces)\n",
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(nfx.remove_stopwords)\n",
    "data['tokenized'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(lambda x: x.split())\n",
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].apply(lambda x: x.lower())\n",
    "data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'] = data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'].astype(str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['application motivate you to shop in-store?'] = data['application motivate you to shop in-store?'].apply(nfx.remove_hashtags)\n",
    "data['application motivate you to shop in-store?'] = data['application motivate you to shop in-store?'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['application motivate you to shop in-store?'] = data['application motivate you to shop in-store?'].apply(nfx.remove_urls)\n",
    "data['application motivate you to shop in-store?'] = data['application motivate you to shop in-store?'].apply(nfx.remove_puncts)\n",
    "data['application motivate you to shop in-store?'] = data['application motivate you to shop in-store?'].apply(nfx.remove_multiple_spaces)\n",
    "data['application motivate you to shop in-store?'] = data['application motivate you to shop in-store?'].apply(nfx.remove_stopwords)\n",
    "data[['application motivate you to shop in-store?', 'application motivate you to shop in-store?']]\n",
    "data['tokenized'] = data['application motivate you to shop in-store?'].apply(lambda x: x.split())\n",
    "data['application motivate you to shop in-store?'] = data['application motivate you to shop in-store?'].astype(str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['explanations helpful?'] = data['explanations helpful?'].apply(nfx.remove_hashtags)\n",
    "data['explanations helpful?'] = data['explanations helpful?'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['explanations helpful?'] = data['explanations helpful?'].apply(nfx.remove_urls)\n",
    "data['explanations helpful?'] = data['explanations helpful?'].apply(nfx.remove_puncts)\n",
    "data['explanations helpful?'] = data['explanations helpful?'].apply(nfx.remove_multiple_spaces)\n",
    "data['explanations helpful?'] = data['explanations helpful?'].apply(nfx.remove_stopwords)\n",
    "data['tokenized'] = data['explanations helpful?'].apply(lambda x: x.split())\n",
    "data['explanations helpful?'] = data['explanations helpful?'].apply(lambda x: x.lower())\n",
    "data['explanations helpful?'] = data['explanations helpful?'].astype(str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['in shopping what is good?'] = data['in shopping what is good?'].apply(nfx.remove_hashtags)\n",
    "data['in shopping what is good?'] = data['in shopping what is good?'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['in shopping what is good?'] = data['in shopping what is good?'].apply(nfx.remove_urls)\n",
    "data['in shopping what is good?'] = data['in shopping what is good?'].apply(nfx.remove_puncts)\n",
    "data['in shopping what is good?'] = data['in shopping what is good?'].apply(nfx.remove_multiple_spaces)\n",
    "data['in shopping what is good?'] = data['in shopping what is good?'].apply(nfx.remove_stopwords)\n",
    "data['tokenized'] = data['in shopping what is good?'].apply(lambda x: x.split())\n",
    "data['in shopping what is good?'] = data['in shopping what is good?'].apply(lambda x: x.lower())\n",
    "data['in shopping what is good?'] = data['in shopping what is good?'].str.lower()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].apply(nfx.remove_hashtags)\n",
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].apply(nfx.remove_urls)\n",
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].apply(nfx.remove_puncts)\n",
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].apply(nfx.remove_multiple_spaces)\n",
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].apply(nfx.remove_stopwords)\n",
    "data['tokenized'] = data['satisfied with brick-and-mortar'].apply(lambda x: x.split())\n",
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].apply(lambda x: x.lower())\n",
    "data['satisfied with brick-and-mortar'] = data['satisfied with brick-and-mortar'].astype(str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].apply(nfx.remove_hashtags)\n",
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].apply(lambda x: nfx.remove_userhandles(x))\n",
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].apply(nfx.remove_urls)\n",
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].apply(nfx.remove_puncts)\n",
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].apply(nfx.remove_multiple_spaces)\n",
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].apply(nfx.remove_stopwords)\n",
    "data[['technologies brick-and-mortar provide in the future?', 'technologies brick-and-mortar provide in the future?']]\n",
    "data['tokenized'] = data['technologies brick-and-mortar provide in the future?'].apply(lambda x: x.split())\n",
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].apply(lambda x: x.lower())\n",
    "data['technologies brick-and-mortar provide in the future?'] = data['technologies brick-and-mortar provide in the future?'].astype(str)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: price comparison ', comparison products ', price comparison products ', comparison different packages\n",
      "Topic #1: think features ', features missed ', think features missed ', cant think\n",
      "Topic #2: comparison shops ', price comparison shops ', comparison shops near ', shops near environment\n",
      "Topic #3: ingredients food ', calories ampelsystem ', ingredients food calories ', food calories\n",
      "Topic #4: shopping list ', add shopping ', add shopping list ', intended buy\n",
      "Topic #5: id like ', product cheaper ', like store product ', store product cheaper\n",
      "Topic #6: product like ', like ingredients ', product like ingredients ', information product\n",
      "Topic #7: comparison stores ', comparison stores price ', stores price ', price comparison stores\n",
      "Topic #8: different stores ', price comparisons ', comparisons different ', comparisons different stores\n",
      "Topic #9: prices compared ', nutritional values ', prices compared prices ', information prices compared\n",
      "Topic #10: produced etc ', eg co2 footprint ', eg co2 ', co2 footprint\n",
      "Topic #11: think covered ', useful think covered ', useful think ', think covered example\n",
      "Topic #12: save product ', similar shoes ', save product later ', purchase kind\n",
      "Topic #13: information price ', price promotions ', information price promotions ', market helpful\n",
      "Topic #14: like product ', product packing ', details like displayed ', displayed product packing\n",
      "Topic #15: presented books ', date expiry ', option healthier options ', food based\n",
      "Topic #16: milk like ', milk like shown ', nice ideas milk ', nice ideas\n",
      "Topic #17: food items ', list barcode scanner ', maybe food items ', ingredient list\n",
      "Topic #18: recommendations based ', recommendations based feature ', eg sugarfree ', vegan whatever\n",
      "Topic #19: think order ', test delete ', clickworker test delete ', clickworker test\n",
      "Topic #20: nein ist ausreichend ', nein ist ', ist ausreichend ', price competition\n",
      "Topic #21: idea moment ', comparing prices ', comparing prices internet ', prices internet\n",
      "Topic #22: features good ', prices stores ', tests nice ', product tests\n",
      "Topic #23: siri etc ', google assistant ', link alexa google ', assistant siri\n",
      "Topic #24: think theres enought ', information decide ', buy item ', think theres\n",
      "Topic #25: detaillied information products ', information products ', detaillied information ', information fairness\n",
      "Topic #26: overloaded information ', data privacy ', online einkauf ', eingebauter online preisvergleich\n",
      "Topic #27: ingredients technical ', technical information ', ingredients technical information ', mir fehlt\n",
      "Topic #28: far features ', think perfect ', item whishlist ', option add item\n",
      "Topic #29: scan place ', select scan ', select scan place ', guide select\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english') + ['miss'] + ['missing']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['features missing?'])\n",
    "def print_top_words1(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words1(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: dont know ', price store ', missing great ', product price\n",
      "Topic #1: dont issues ', systems hacked ', compatible systems ', photo instead\n",
      "Topic #2: use app ', dont use ', wouldnt use ', use smarphone\n",
      "Topic #3: product information ', detailled product ', shopping list ', bought neutral\n",
      "Topic #4: looks good ', makes hard ', hard enjoy ', good interface\n",
      "Topic #5: customer needs ', needs product ', looks bit ', mundane items\n",
      "Topic #6: short information ', information clicking ', summary scanning ', clicking information\n",
      "Topic #7: similar products ', lack balance ', balance naming ', naming similar\n",
      "Topic #8: social media ', possible disconnected ', disconnected social ', issue social\n",
      "Topic #9: price comparison ', comparison better ', design app ', better design\n",
      "Topic #10: price sale ', store product ', said store ', yes said\n",
      "Topic #11: product app ', looks wobbly ', identifying product ', wobbly annoying\n",
      "Topic #12: information screen ', partly information ', think partly ', getting sick\n",
      "Topic #13: scan products ', want look ', scanning product ', taking pictures\n",
      "Topic #14: scan product ', products dark ', wrong cause ', stores products\n",
      "Topic #15: information want ', choose kind ', interesting user ', user choose\n",
      "Topic #16: alternative products ', purpose shoes ', labeled especially ', products shown\n",
      "Topic #17: price product ', recommendation offers ', product recommendation ', division price\n",
      "Topic #18: app user ', store store ', user slowly ', wanting buy\n",
      "Topic #19: information product ', improved hud ', issues room ', design innovative\n",
      "Topic #20: product bought ', know things ', want know ', place want\n",
      "Topic #21: real person ', items look ', eg shoes ', look real\n",
      "Topic #22: dont improvements ', information users ', users surface ', lot information\n",
      "Topic #23: dont think ', cant think ', design better ', important issuse\n",
      "Topic #24: infos screen ', perhapst infos ', easier differentiate ', use colors\n",
      "Topic #25: simple design ', social things ', litte bit ', looks litte\n",
      "Topic #26: dont problems ', transparent products ', products recommended ', think game\n",
      "Topic #27: für empfehlungen ', man swipen ', wie kunden ', kauften auch\n",
      "Topic #28: nein im ', im moment ', moment nicht ', think easy\n",
      "Topic #29: informations basics ', room issues ', app look ', look pleasing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning:\n",
      "\n",
      "Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english') + ['find'] + ['like']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,2))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['room for improvement?'])\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: dont touch ', yes dont touch ', yes dont ', dont touch products\n",
      "Topic #1: dont know ', dont know stuff ', know stuff ', takes time\n",
      "Topic #2: need touch ', need touch products ', dont need ', dont need touch\n",
      "Topic #3: dont think ', think need ', dont think need ', think need shop\n",
      "Topic #4: yes contact ', yes contact vendors ', contact vendors ', yes contact people\n",
      "Topic #5: shop assistant ', talk shop ', need talk shop ', talk shop assistant\n",
      "Topic #6: shopping trip ', trip secure ', shopping trip secure ', think shopping trip\n",
      "Topic #7: touch product ', product read ', touch product read ', maybe touch product\n",
      "Topic #8: yes need touch ', yes need ', need touch ', touch items\n",
      "Topic #9: longer shop ', stay longer shop ', stay longer ', risk infected\n",
      "Topic #10: wouldnt touch ', wouldnt touch products ', touch products ', maybe wouldnt touch\n",
      "Topic #11: online shopping ', prefer online ', prefer online shopping ', yes online\n",
      "Topic #12: contact people ', direct contact ', direct contact people ', sure hygienic\n",
      "Topic #13: shopping trips secure ', trips secure ', shopping trips ', trips secure covid19\n",
      "Topic #14: use app ', app statement ', use app statement ', persons use\n",
      "Topic #15: shopping faster ', yes shopping ', yes shopping faster ', maybe use shopping\n",
      "Topic #16: im sure ', shop faster ', speeding shopping ', faster tool\n",
      "Topic #17: spend time ', time comparing ', spend time comparing ', yes course\n",
      "Topic #18: contact product ', product yes ', contact product yes ', product touching\n",
      "Topic #19: want buy ', maybe want ', maybe want buy ', buy product\n",
      "Topic #20: ja ich ', kontaktlos einkaufen ', ich kann ', kann kontaktlos einkaufen\n",
      "Topic #21: yes interaction ', yes interaction clerks ', interaction clerks ', custumer support needed\n",
      "Topic #22: risk infection ', yes lower risk ', yes lower ', lower risk\n",
      "Topic #23: shopping safer ', sure shopping ', sure shopping safer ', safer dont\n",
      "Topic #24: wouldnt need ', wouldnt need touch ', need touch ', touch things\n",
      "Topic #25: covid 19 ', think useful ', yes think ', covid 19 reason\n",
      "Topic #26: yes buy ', buy things ', things online ', buy things online\n",
      "Topic #27: nein ich ', ich würde ', im laden ', zeit im\n",
      "Topic #28: yes avoid ', avoid physical contacts ', physical contacts ', avoid physical\n",
      "Topic #29: social distancing ', shopping social ', shopping social distancing ', talk shopping social\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english') + ['miss'] + ['missing']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['Would this application help to make your shopping trip more secure during COVID19? If yes, why and if no, why not?'])\n",
    "def print_top_words1(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words1(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: yes information ', yes information helpful ', information easily ', yes information easily\n",
      "Topic #1: yes easier ', yes choices ', long corona ', corona pandemic\n",
      "Topic #2: prefere online ', online store ', prefere online store ', online store smartphone\n",
      "Topic #3: dont need ', dont need application ', need application ', im instore\n",
      "Topic #4: like online ', online stores ', like online stores ', stores motivating\n",
      "Topic #5: yes fun ', fun use ', yes fun use ', search information\n",
      "Topic #6: dont like ', like offline ', moving pictures ', like juggly\n",
      "Topic #7: prefer online ', use app ', wouldnt use ', wouldnt use app\n",
      "Topic #8: information online ', online home ', information online home ', yes provides information\n",
      "Topic #9: buy need ', need short trips ', realy buy ', buy need short\n",
      "Topic #10: yes interesting ', new technology ', try new technology ', try new\n",
      "Topic #11: yes gives ', lot informations ', information product ', gives lot informations\n",
      "Topic #12: prefer stores ', stores online ', prefer stores online ', woudnt change\n",
      "Topic #13: online easier ', see question ', detailed see ', detailed see question\n",
      "Topic #14: information available ', available products ', information available products ', internet information\n",
      "Topic #15: relevant information ', information products ', yes relevant ', yes relevant information\n",
      "Topic #16: yes better ', better overview ', yes better overview ', better informatios products\n",
      "Topic #17: change motivation ', additional information ', dont think ', product actuall\n",
      "Topic #18: easy use ', buy things ', looks easy ', looks easy use\n",
      "Topic #19: yes motivate ', yes motivate instore ', motivate instore ', provides information\n",
      "Topic #20: product information ', product information stuff ', prefer product information ', prefer product\n",
      "Topic #21: instore information ', shoes clothing aswell ', clothing aswell ', information online product\n",
      "Topic #22: spend time ', need buy ', time store scanning ', item need\n",
      "Topic #23: want fast ', like store ', time want fast ', takes time\n",
      "Topic #24: compare prices ', quickly compare ', quickly compare prices ', good quickly\n",
      "Topic #25: use application ', real use ', use application feel ', use application online\n",
      "Topic #26: yes groceries ', price comparison ', maybe price ', maybe price comparison\n",
      "Topic #27: ich habe ', ja ich ', mühsam einen ', infos zur\n",
      "Topic #28: yes makes ', makes faster ', yes makes faster ', interesting gather\n",
      "Topic #29: read product ', product screen ', want read ', read product screen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english') + ['shop'] + ['shopping']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['application motivate you to shop in-store?'])\n",
    "def print_top_words1(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words1(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: yes helpful ', yes helpful easy ', yes helpful informations ', recomandations products\n",
      "Topic #1: yes easy ', yes easy understand ', easy understand ', easy understand helpful\n",
      "Topic #2: yes information ', yes information helpful ', information helpful ', think yes information\n",
      "Topic #3: yes shows ', yes shows want ', want know product ', know product\n",
      "Topic #4: yes detailed ', good job ', yeah good ', yeah good job\n",
      "Topic #5: yes clear ', yes clear concise ', clear concise ', clear let\n",
      "Topic #6: yes explained ', yes explained illustrated ', explained illustrated ', symbols shown\n",
      "Topic #7: yes relevant ', relevant information ', yes relevant information ', especially grocery\n",
      "Topic #8: yes clearly ', yes clearly written ', clearly written ', ja weil\n",
      "Topic #9: yes like ', like incredients list ', like incredients ', incredients list\n",
      "Topic #10: yes explanations ', explanations simple ', yes explanations simple ', explanations sense\n",
      "Topic #11: yes short ', yes short point ', yes short factual ', short factual\n",
      "Topic #12: easy understand ', helpful easy understand ', helpful easy ', yes helpful easy\n",
      "Topic #13: yes especially ', especially recommendations ', yes especially recommendations ', recommendations based\n",
      "Topic #14: information product ', yes gives ', gives information ', yes gives information\n",
      "Topic #15: explanations helpful ', good overview ', helpful good overview ', explanations helpful good\n",
      "Topic #16: yes understand ', understand app ', yes understand app ', understand product function\n",
      "Topic #17: find helpful ', yes find ', yes find helpful ', find helpful intuitive\n",
      "Topic #18: yes important ', important details ', yes important details ', information available\n",
      "Topic #19: dont need ', information dont ', person ask ', dont need person\n",
      "Topic #20: want buy ', buy milk ', want buy milk ', social media\n",
      "Topic #21: given information ', usefull understood easily ', information usefull ', information usefull understood\n",
      "Topic #22: yes helps ', helps purchase decision ', helps use app ', helps purchase\n",
      "Topic #23: ja macht ', es verständlicher ', ja macht es ', macht es\n",
      "Topic #24: yes different ', different informations given ', different informations ', informations given\n",
      "Topic #25: easy use ', yes easy use ', selfexplanatory looks easy ', think selfexplanatory\n",
      "Topic #26: additional information ', product like ', advantages product ', information advantages\n",
      "Topic #27: yes price ', book recommendations ', yes price helpful ', price helpful book\n",
      "Topic #28: feature based ', like id ', feature based information ', id research\n",
      "Topic #29: basically yes ', reliable see question ', basically yes data ', reliable see\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['explanations helpful?'])\n",
    "def print_top_words1(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words1(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: wouldnt use ', shops supermarkets ', clothing shops supermarkets ', clothing shops\n",
      "Topic #1: use application ', likely use application ', likely use ', application electronics\n",
      "Topic #2: grocery shopping ', shopping quickly ', grocery shopping thats ', shopping thats\n",
      "Topic #3: grocery electronics ', luxury clothing ', grocery electronics luxury ', electronics luxury clothing\n",
      "Topic #4: electronics need ', electronics need informations ', need informations ', informations products\n",
      "Topic #5: electronics reviews ', reviews helpful ', electronics reviews helpful ', electronics reviews products\n",
      "Topic #6: find best product ', best product ', find best ', electronics important find\n",
      "Topic #7: use app ', app shopping ', use app shopping ', likely use\n",
      "Topic #8: grocery store ', use grocery ', use grocery store ', nutrition facts\n",
      "Topic #9: maybe clothing ', clothing simular ', simular products ', maybe clothing simular\n",
      "Topic #10: probably electronics ', information products ', relevant information ', find relevant\n",
      "Topic #11: grocery products ', food products ', ingredients information ', contain different\n",
      "Topic #12: use expensive ', use expensive products ', expensive products ', products buy\n",
      "Topic #13: electronics price ', price variys online ', electronics price variys ', variys online\n",
      "Topic #14: compare different ', electronics compare ', buying electronics ', different products\n",
      "Topic #15: electronics books ', maybe electronics ', groceries electronics ', groceries electronics books\n",
      "Topic #16: electronics technical ', technical information ', information comparison ', technical information comparison\n",
      "Topic #17: wont use ', necessary information ', wont use thing ', living person\n",
      "Topic #18: grocery clothing ', informations ingredients origin ', clothing background ', clothing background informations\n",
      "Topic #19: electronics dont ', electronics dont know ', dont know ', books electronics\n",
      "Topic #20: prefer use ', use buying ', want check ', item origin fabric\n",
      "Topic #21: electronics things ', clothing electronics ', makes easier ', think app\n",
      "Topic #22: electronics probably ', probably save ', save money ', probably save money\n",
      "Topic #23: find information ', things buy ', find information things ', information things buy\n",
      "Topic #24: shopping scenario ', information shopping ', shopping scenario information ', information shopping faster\n",
      "Topic #25: luxury electronics ', stuff want ', luxury electronics item ', electronics item recommendation\n",
      "Topic #26: bei elektronik ', hauptsächlich bei elektronik ', allen bereichen aber ', bereichen aber\n",
      "Topic #27: electronics etc ', app help ', want buy ', likely complex\n",
      "Topic #28: elektronik da ', technische infos ', wichtig sind ', da hier technische\n",
      "Topic #29: electronics clothing ', like use ', shopping electronics ', use grocery\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning:\n",
      "\n",
      "Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['in shopping what is good?'])\n",
    "def print_top_words1(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words1(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: dont know ', dont know label ', dont know kind ', know label\n",
      "Topic #1: yes good ', yes good idea ', good idea ', good experiences\n",
      "Topic #2: online shopping ', prefer online shopping ', prefer online ', covid prefer online\n",
      "Topic #3: yes like ', like general ', like shop ', yes like shop\n",
      "Topic #4: dont use ', neutral dont use ', neutral dont ', use tthose\n",
      "Topic #5: yes im ', im good ', yes im good ', yes im satisfied\n",
      "Topic #6: yes easy ', easy use ', yes easy use ', easy handle\n",
      "Topic #7: brickandmortar services ', brickandmortar services clue ', familiar brickandmortar ', heard brickandmortar\n",
      "Topic #8: yes satisfied ', satisfied selections ', satisfied selections products ', yes satisfied selections\n",
      "Topic #9: like touch ', touch products ', like touch products ', yes like touch\n",
      "Topic #10: dont care ', dont care services ', dont care way ', care services\n",
      "Topic #11: dont need ', neutral dont ', neutral dont need ', depends dont need\n",
      "Topic #12: ich bin zufrieden ', ich bin ', bin zufrieden ', ja ich\n",
      "Topic #13: im satisfied ', think im satisfied ', think im ', satisfied services\n",
      "Topic #14: yes generally ', usually good overview ', services usually good ', brickandmortar services usually\n",
      "Topic #15: yes provides ', important informations ', provides good ', good information\n",
      "Topic #16: im sure ', products best interest ', stuff im sure ', products best\n",
      "Topic #17: yes better ', yes better consulting ', better consulting ', want know\n",
      "Topic #18: yes prefer ', prefer reality ', yes prefer reality ', touch product expect\n",
      "Topic #19: shop shops ', general shop shops ', general shop ', like shopping\n",
      "Topic #20: yes product ', product immediately ', buying wait ', yes product buying\n",
      "Topic #21: human contact ', yes ok personal ', personal human ', ok personal human\n",
      "Topic #22: ja da ', nachfragen kann ', man nachfragen ', ja da man\n",
      "Topic #23: getting information ', read description ', information read ', yes getting information\n",
      "Topic #24: good idea ', shopping experience ', good idea future ', thats good\n",
      "Topic #25: products moment ', online shops ', prefer online shops ', prefer online\n",
      "Topic #26: neutral services ', yes missed ', yes straightforward ', store good\n",
      "Topic #27: near product ', yes near product ', yes near ', counseling better\n",
      "Topic #28: things like ', touch things ', touch things like ', yes want\n",
      "Topic #29: yes usually ', range products ', working know ', know range products\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['satisfied with brick-and-mortar'])\n",
    "def print_top_words1(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words1(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning:\n",
      "\n",
      "The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: dont know ', know thought ', dont know thought ', im excited\n",
      "Topic #1: im sure ', im sure sorry ', sure sorry ', special mind\n",
      "Topic #2: digital shopping ', shopping experiences ', augmented reality digital ', reality digital shopping\n",
      "Topic #3: home delivery ', home delivery groceries ', onlineshopping home delivery ', onlineshopping home\n",
      "Topic #4: dontt know ', know think ok ', know think ', think ok\n",
      "Topic #5: know maybe ', dont know maybe ', know maybe apple ', know maybe robotics\n",
      "Topic #6: augmented reality ', augmented reality apps ', reality apps ', reality tools\n",
      "Topic #7: virtual reality ', service augmented virtual ', augmented virtual reality ', augmented virtual\n",
      "Topic #8: technologies like ', selfcheckout etc combined ', selfservice technologies like ', etc combined app\n",
      "Topic #9: delivery service ', delivery service stores ', service stores ', shop delivery\n",
      "Topic #10: cant think ', technology outofthebox thinking ', thinking actual person ', service staff\n",
      "Topic #11: pick store ', online shopping ', shopping pick store ', online shopping pick\n",
      "Topic #12: currently stock ', provide information ', available currently stock ', provide information product\n",
      "Topic #13: systems like ', cashing systems like ', systems like denmark ', like denmark\n",
      "Topic #14: customer service ', service delivery ', customer service delivery ', online customer\n",
      "Topic #15: app good ', good idea ', shown app ', like shown app\n",
      "Topic #16: think technology ', technology difficult use ', use app ', difficult use app\n",
      "Topic #17: information online ', online know products ', products availability shop ', availability shop\n",
      "Topic #18: store check ', online store check ', check availability ', availability prices\n",
      "Topic #19: stores provide ', provide technologies ', buying things ', thinks neccessary\n",
      "Topic #20: price offers ', products online ', offering products online ', offering products\n",
      "Topic #21: hand sanitizers ', construction printing ', home construction printing ', home construction\n",
      "Topic #22: electronic pay ', informations products ', digital informations ', digital informations products\n",
      "Topic #23: paying smartphones ', keine ahnung ', helpful shoppingassistent ', helpful shoppingassistent phone\n",
      "Topic #24: delivery services ', need fair ', working conditions ', need fair working\n",
      "Topic #25: augumented reality ', recommendation systems ', included way ', systems included\n",
      "Topic #26: technologies needed ', find product ', information signs ', signs find product\n",
      "Topic #27: similiar app ', presented app ', possible ones ', depends store\n",
      "Topic #28: interactive games ', unfortunately idea ', app useful ', special mind\n",
      "Topic #29: automaticly scanning paying ', exit store ', paying exit ', paying exit store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=30)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(data['technologies brick-and-mortar provide in the future?'])\n",
    "def print_top_words1(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" ', \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words1(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
